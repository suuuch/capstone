{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms, models\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:397: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n"
     ]
    }
   ],
   "source": [
    "path = \"./data\"\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "                             # transforms.Lambda(shear),\n",
    "                             transforms.RandomSizedCrop(224),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize,\n",
    "                                 ])\n",
    "\n",
    "data_image = {x:datasets.ImageFolder(root = os.path.join(path,x),\n",
    "                                     transform = transform)\n",
    "              for x in [\"train\", \"val\"]}\n",
    "\n",
    "data_loader_image = {x:torch.utils.data.DataLoader(dataset=data_image[x],\n",
    "                                                num_workers=4,\n",
    "                                                batch_size=8,\n",
    "                                                pin_memory=True,\n",
    "                                                shuffle = True)\n",
    "                     for x in [\"train\", \"val\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog']\n",
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "classes = data_image[\"train\"].classes\n",
    "classes_index = data_image[\"train\"].class_to_idx\n",
    "print(classes)\n",
    "print(classes_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集个数: 22500\n",
      "验证集个数: 2500\n"
     ]
    }
   ],
   "source": [
    "print(u\"训练集个数:\", len(data_image[\"train\"]))\n",
    "print(u\"验证集个数:\", len(data_image[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = next(iter(data_loader_image[\"train\"]))\n",
    "mean = [0.5,0.5,0.5]\n",
    "std  = [0.5,0.5,0.5]\n",
    "img = torchvision.utils.make_grid(X_train)\n",
    "img = img.numpy().transpose((1,2,0))\n",
    "img = img*std+mean\n",
    "\n",
    "print([classes[i] for i in y_train])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "for parma in model.parameters():\n",
    "    parma.requires_grad = False\n",
    "\n",
    "model.classifier = torch.nn.Sequential(torch.nn.Linear(25088, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 2))\n",
    "\n",
    "for index, parma in enumerate(model.classifier.parameters()):\n",
    "    if index == 6:\n",
    "        parma.requires_grad = True\n",
    "    \n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(),lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0/2\n",
      "----------\n",
      "Batch 500, Train Loss:151.1232, Train ACC:182.2000\n",
      "Batch 1000, Train Loss:145.9629, Train ACC:182.3750\n",
      "Batch 1500, Train Loss:151.4611, Train ACC:182.4667\n",
      "Batch 2000, Train Loss:141.7430, Train ACC:183.0375\n",
      "Batch 2500, Train Loss:143.9511, Train ACC:182.6500\n",
      "train  Loss: 70.3320,  Correct: 91.4800\n",
      "val  Loss: 70.3288,  Correct: 92.2000\n",
      "Training time is:2m 4s\n",
      "Epoch1/2\n",
      "----------\n",
      "Batch 500, Train Loss:126.0623, Train ACC:183.6000\n",
      "Batch 1000, Train Loss:154.0388, Train ACC:184.0000\n",
      "Batch 1500, Train Loss:145.3600, Train ACC:184.3500\n",
      "Batch 2000, Train Loss:151.4245, Train ACC:184.0250\n",
      "Batch 2500, Train Loss:168.2391, Train ACC:184.1700\n",
      "train  Loss: 83.3328,  Correct: 92.0178\n",
      "val  Loss: 110.4827,  Correct: 92.6000\n",
      "Training time is:2m 5s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    since = time.time()\n",
    "    print(\"Epoch{}/{}\".format(epoch, n_epochs))\n",
    "    print(\"-\"*10)\n",
    "    for param in [\"train\", \"val\"]:\n",
    "        if param == \"train\":\n",
    "            model.train = True\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        else:\n",
    "            model.train = False\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0 \n",
    "        batch = 0\n",
    "        for data in data_loader_image[param]:\n",
    "            batch += 1\n",
    "            X, y = data\n",
    "            if use_gpu:\n",
    "                X, y  = Variable(X.cuda()), Variable(y.cuda())\n",
    "            else:\n",
    "                X, y = Variable(X), Variable(y)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            _, pred = torch.max(y_pred.data, 1)\n",
    "        \n",
    "            loss = cost(y_pred, y)\n",
    "            if param ==\"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "            running_correct += torch.sum(pred == y.data)\n",
    "            if batch%500 == 0 and param ==\"train\":\n",
    "                print(\"Batch {}, Train Loss:{:.4f}, Train ACC:{:.4f}\".format(\n",
    "                      batch, running_loss/(4*batch), 100*running_correct/(4*batch)))\n",
    "            \n",
    "        epoch_loss = running_loss/len(data_image[param])\n",
    "        epoch_correct = 100*running_correct/len(data_image[param])\n",
    "\n",
    "        print(\"{}  Loss: {:.4f},  Correct: {:.4f}\".format(param, epoch_loss, epoch_correct))\n",
    "    now_time = time.time() - since   \n",
    "    print(\"Training time is:{:.0f}m {:.0f}s\".format(now_time//60, now_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_vgg16_finetune2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_vgg16_finetune2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_img = ImageFolder(root=\"./test/\",\n",
    "                                     transform = transform)\n",
    "data_loader_test_img = torch.utils.data.DataLoader(dataset=data_test_img,\n",
    "                                                   batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( 0 , 0 ,.,.) = \n",
      " -2.0323 -2.0152 -1.9467  ...  -1.6384 -1.6213 -1.6042\n",
      " -2.0494 -2.0665 -2.0494  ...  -1.6555 -1.6727 -1.6727\n",
      " -2.0152 -1.9980 -1.9809  ...  -1.6898 -1.7069 -1.7069\n",
      "           ...             ⋱             ...          \n",
      " -0.6794 -0.7308 -0.5596  ...   0.7591  0.8276  0.8618\n",
      " -0.5424 -0.6623 -0.5596  ...   0.7419  0.7933  0.8276\n",
      " -0.5082 -0.4911 -0.1314  ...   0.7248  0.7591  0.7933\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -1.7381 -1.7206 -1.6681  ...  -1.2129 -1.1954 -1.1779\n",
      " -1.8256 -1.8606 -1.8782  ...  -1.1954 -1.1954 -1.1954\n",
      " -1.8256 -1.8431 -1.8606  ...  -1.2129 -1.2129 -1.2129\n",
      "           ...             ⋱             ...          \n",
      " -0.8277 -0.9328 -0.7927  ...   0.9405  1.0105  1.0455\n",
      " -0.5126 -0.6877 -0.6176  ...   0.9230  0.9755  1.0105\n",
      " -0.4076 -0.4076 -0.0924  ...   0.9055  0.9405  0.9755\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -1.0201 -0.9853 -0.8981  ...  -0.2010 -0.1661 -0.1487\n",
      " -1.2467 -1.2641 -1.2467  ...  -0.1661 -0.1487 -0.1487\n",
      " -1.3339 -1.3164 -1.3339  ...  -0.2184 -0.2010 -0.1835\n",
      "           ...             ⋱             ...          \n",
      " -1.0550 -1.1247 -0.9678  ...   1.1411  1.2108  1.2457\n",
      " -0.7761 -0.8981 -0.8284  ...   1.1237  1.1759  1.2108\n",
      " -0.6367 -0.6018 -0.2707  ...   1.1062  1.1411  1.1759\n",
      "[torch.FloatTensor of size 1x3x224x224]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "('./test/test/1.jpg',)\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in data_loader_test_img:\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "clip = 0.001\n",
    "cnt = 1\n",
    "csv_map = {}\n",
    "\n",
    "for image, label, path in (data_loader_test_img):\n",
    "    images = Variable(image.cuda())\n",
    "    y_pred = model(images)\n",
    "    smax = torch.nn.Softmax()\n",
    "    smax_out = smax(y_pred)[0]\n",
    "    cat_prob = smax_out.data[0]\n",
    "    dog_prob = smax_out.data[1]\n",
    "    prob = dog_prob\n",
    "\n",
    "    if cat_prob > dog_prob:\n",
    "        prob = 1 - cat_prob\n",
    "    prob = np.around(prob, decimals=4)\n",
    "    prob = np.clip(prob, clip, 1-clip)\n",
    "    filepath = path[0].split('/')[-1].split('.')[-2]\n",
    "    if filepath == '0':\n",
    "        print(path)\n",
    "        break\n",
    "    csv_map[filepath] = prob\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list = []\n",
    "for k,v in csv_map.items():\n",
    "    csv_list.append((k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(csv_list)\n",
    "df.columns = ['id','label']\n",
    "df.id = df.id.astype(int)\n",
    "df = df.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(image)\n",
    "img = img.numpy().transpose(1,2,0)\n",
    "mean = [0.5,0.5,0.5]\n",
    "std  = [0.5,0.5,0.5]\n",
    "img = img*std+mean\n",
    "print(\"Pred Label:\",[classes[i] for i in pred])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
