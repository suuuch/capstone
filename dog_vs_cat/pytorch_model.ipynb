{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of Dogs vs. Cats With PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import collections\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = DATA_HOME_DIR + '/' \n",
    "split_train_path = data_path + '/train/'\n",
    "full_train_path = data_path + '/train_full/'\n",
    "valid_path = data_path + '/valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/test/'\n",
    "saved_model_path = ROOT_DIR + '/models/'\n",
    "submission_path = ROOT_DIR + '/submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split file to Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_name(floder,ratio=0.5):\n",
    "    file_list = os.listdir(floder)\n",
    "    \n",
    "\n",
    "    dog_jpg = list(filter(lambda x : 'dog' in x  , file_list))\n",
    "    cat_jpg = list(filter(lambda x : 'cat' in x  , file_list))\n",
    "\n",
    "    print('In the %s floder , All Jpg file is %s ;' % (floder, len(file_list)))\n",
    "    print('In the %s floder , Dog jpg file is %s ;' % (floder, len(dog_jpg)))\n",
    "    print('In the %s floder , Cat Jpg file is %s ;' % (floder, len(cat_jpg)))\n",
    "        \n",
    "    #计算数据划分的训练集和验证集的分割点\n",
    "    n_sample = len(file_list)\n",
    "    n_val = math.ceil(n_sample / 2 * (ratio + 0.1)) # number of validation samples\n",
    "    n_train = math.ceil(n_sample / 2 * (1 - ratio + 0.1)) # number of trainning samples\n",
    "    \n",
    "    #依据分割点划分数据\n",
    "    # 训练集和验证集在划分的时候，会有一定量的重复。\n",
    "    tra_images = random.sample(dog_jpg, n_train)\n",
    "    tra_images.extend(random.sample(cat_jpg, n_train))\n",
    "    \n",
    "    val_images = dog_jpg[n_train:]\n",
    "    val_images.extend(dog_jpg[n_train:])\n",
    "    \n",
    "    return tra_images,val_images\n",
    "\n",
    "def split_files_to_train_val():\n",
    "    try:\n",
    "        os.mkdir(full_train_path)\n",
    "        os.mkdir(valid_path)\n",
    "    except FileExistsError as e:\n",
    "        print(e)\n",
    "        \n",
    "    tra_images,val_images = get_all_files_name(split_train_path,0.3)\n",
    "    \n",
    "    print('tra images : %s ' % len(tra_images))\n",
    "    print('val images: %s' % len(val_images))\n",
    "    \n",
    "    for img in set(tra_images):\n",
    "        dst = os.path.join(split_train_path,img)\n",
    "        src = os.path.join(full_train_path,img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    for img in set(val_images):\n",
    "        src = os.path.join(split_train_path,img)\n",
    "        dst = os.path.join(valid_path,img)\n",
    "        shutil.copyfile(src, dst)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_files_to_train_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data\n",
    "batch_size = 8\n",
    "nb_split_train_samples = 23000\n",
    "nb_full_train_samples = 25000\n",
    "nb_valid_samples = 2000\n",
    "nb_test_samples = 12500\n",
    "\n",
    "# model\n",
    "nb_runs = 1\n",
    "nb_aug = 3\n",
    "epochs = 35\n",
    "lr = 1e-4\n",
    "clip = 0.001\n",
    "archs = [\"resnet152\"]\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__ if name.islower() and not name.startswith(\"__\"))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        if use_gpu:\n",
    "            target = target.cuda(async=True)\n",
    "        image_var = torch.autograd.Variable(images)\n",
    "        label_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec1 = accuracy(y_pred.data, target, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        if use_gpu:\n",
    "            labels = labels.cuda(async=True)\n",
    "            \n",
    "        image_var = torch.autograd.Variable(images, volatile=True)\n",
    "        label_var = torch.autograd.Variable(labels, volatile=True)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, temp_var = accuracy(y_pred.data, labels, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('   * EPOCH {epoch} | Accuracy: {acc.avg:.3f} | Loss: {losses.avg:.3f}'.format(epoch=epoch,\n",
    "                                                                                         acc=acc,\n",
    "                                                                                         losses=losses))\n",
    "\n",
    "    return acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model):\n",
    "    csv_map = collections.defaultdict(float)\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    for aug in range(nb_aug):\n",
    "        print(\"   * Predicting on test augmentation {}\".format(aug + 1))\n",
    "        \n",
    "        for i, (images, filepath) in enumerate(test_loader):\n",
    "            # pop extension, treat as id to map\n",
    "            filepath = os.path.splitext(os.path.basename(filepath[0]))[0]\n",
    "            filepath = int(filepath)\n",
    "\n",
    "            image_var = torch.autograd.Variable(images, volatile=True)\n",
    "            y_pred = model(image_var)\n",
    "            # get the index of the max log-probability\n",
    "            smax = nn.Softmax()\n",
    "            smax_out = smax(y_pred)[0]\n",
    "            cat_prob = smax_out.data[0]\n",
    "            dog_prob = smax_out.data[1]\n",
    "            prob = dog_prob\n",
    "            if cat_prob > dog_prob:\n",
    "                prob = 1 - cat_prob\n",
    "            prob = np.around(prob, decimals=4)\n",
    "            prob = np.clip(prob, clip, 1-clip)\n",
    "            csv_map[filepath] += (prob / nb_aug)\n",
    "\n",
    "    sub_fn = submission_path + '{0}epoch_{1}clip_{2}runs'.format(epochs, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "        \n",
    "    print(\"Writing Predictions to CSV...\")\n",
    "    with open(sub_fn + '.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'label']\n",
    "        csv_w = csv.writer(csvfile)\n",
    "        csv_w.writerow(('id', 'label'))\n",
    "        for row in sorted(csv_map.items()):\n",
    "            csv_w.writerow(row)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    global lr\n",
    "    lr = lr * (0.1**(epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_actual, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = y_actual.size(0)\n",
    "\n",
    "    _, pred = y_pred.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageFolder(data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        images = []\n",
    "        for filename in sorted(glob.glob(test_path + \"*.jpg\")):\n",
    "            images.append('{}'.format(filename))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear(img):\n",
    "    width, height = img.size\n",
    "    m = random.uniform(-0.05, 0.05)\n",
    "    xshift = abs(m) * width\n",
    "    new_width = width + int(round(xshift))\n",
    "    img = img.transform((new_width, height), Image.AFFINE,\n",
    "                        (1, m, -xshift if m > 0 else 0, 0, 1, 0),\n",
    "                        Image.BICUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(mode=\"train\", resume=False):\n",
    "    \n",
    "    global best_prec1\n",
    "    \n",
    "    for arch in archs:\n",
    "\n",
    "        # create model\n",
    "        print(\"=> Starting {0} on '{1}' model\".format(mode, arch))\n",
    "        model = models.__dict__[arch](pretrained=True)\n",
    "        # Don't update non-classifier learned features in the pretrained networks\n",
    "        # 在 预先准备好的网络中不需要更新非分类学习特征\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the last fully-connected layer\n",
    "        # 替换最后的全连接层\n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        # 新构造的模型包含默认值 requires_grad=True\n",
    "        # Final dense layer needs to replaced with the previous out chans, and number of classes\n",
    "        # 输出层需要替换之前的 classes 数量 \n",
    "        # in this case -- resnet 101 - it's 2048 with two classes (cats and dogs)\n",
    "        # 这个阳历中 resent 101 中 ，两个class 都包含2048个输入参数\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "\n",
    "        if arch.startswith('alexnet') or arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "            \n",
    "        # optionally resume from a checkpoint\n",
    "        # 从保存的检查点恢复\n",
    "        if resume:\n",
    "            if os.path.isfile(resume):\n",
    "                print(\"=> Loading checkpoint '{}'\".format(resume))\n",
    "                checkpoint = torch.load(resume)\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_prec1 = checkpoint['best_prec1']\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "                print(\"=> Loaded checkpoint (epoch {})\".format(checkpoint['epoch']))\n",
    "            else:\n",
    "                print(\"=> No checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        # Data loading code\n",
    "        # 载入数据\n",
    "        traindir = data_path\n",
    "        valdir = data_path\n",
    "        testdir = data_path\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        print(traindir)\n",
    "        train_loader = data.DataLoader(\n",
    "            datasets.ImageFolder(traindir,\n",
    "                                 transforms.Compose([\n",
    "                                     # transforms.Lambda(shear),\n",
    "                                     transforms.RandomSizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize,\n",
    "                                 ])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "\n",
    "        val_loader = data.DataLoader(\n",
    "            datasets.ImageFolder(valdir,\n",
    "                                 transforms.Compose([\n",
    "                                     transforms.Scale(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize,\n",
    "                                 ])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "\n",
    "        test_loader = data.DataLoader(\n",
    "            TestImageFolder(testdir,\n",
    "                            transforms.Compose([\n",
    "                                # transforms.Lambda(shear),\n",
    "                                transforms.Scale(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize,\n",
    "                            ])),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=False)\n",
    "        \n",
    "        \n",
    "        if mode == \"test\":\n",
    "            test(test_loader, model)\n",
    "            return\n",
    "        \n",
    "        # define loss function (criterion) and pptimizer\n",
    "        # 定义损失函数\n",
    "        if use_gpu:\n",
    "            criterion = nn.CrossEntropyLoss().cuda()\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if mode == \"validate\":\n",
    "            validate(val_loader, model, criterion, 0)\n",
    "            return\n",
    "\n",
    "        optimizer = optim.Adam(model.module.fc.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "            # train for one epoch\n",
    "            train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "            # remember best Accuracy and save checkpoint\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "            }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(mode=\"validate\", resume='model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(mode=\"test\", resume='model_best.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
